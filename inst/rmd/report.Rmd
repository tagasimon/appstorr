---
params:
  data: data
  response: response
  set_title: report_title
title: "`r params$set_title`"
---

```{r global_options, include=FALSE}

pkgs <- c("tidyverse", "Amelia", "rebus", "lubridate","patchwork","ggthemes","knitr","emo","wordcloud", "rmarkdown", "data.table","igraph","ggraph","widyr")

invisible(lapply(pkgs, library, character.only = TRUE))

## Set knitr options
opts_chunk$set(
  fig.width = 14,
  fig.height = 10,
  echo = FALSE,
  results = "asis",
  warning = TRUE
)

## Get user data
data <- params$data
report_config <- params$report_config
response <- params$response

```

# PART 1

## 1. Missingness Map
Some of the Columns are missing some observations for obvious reasons e.g The company doesn't reply to every single review and thus the column will miss some data.

```{r}
reviews <- data
missmap(reviews, col = c("Black", "Yellow"))
```


<!-- ## 2. Some Basic Cleanup and Processing -->
<!-- Let's extract the year, months and major version numbers in into separate columns, will be helpful for further analysis down the road.  -->

```{r}
pattern <- DGT %R% optional(DGT)

reviews_processed <- reviews %>% 
        # na.omit(reviewCreatedVersion) %>% 
        mutate(version_extracted = str_extract(reviewCreatedVersion, pattern = pattern)) %>%
        mutate(version_nmbr = as.numeric(version_extracted)) %>% 
        mutate(year = year(at)
               # month = month(at, label = TRUE), 
               # week_day = wday(at, label = TRUE)
               )

```

## 2. What are the Most Common Used Words in the Reviews?
Top 20 most common words in the reviews 

```{r}
reviews_processed %>% 
  unnest_tokens(word, content) %>% 
  anti_join(stop_words, by="word") %>%  
  count(word, sort = TRUE) %>% 
  head(20) %>% 
  mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  labs(x="", y="Count")

```


## 3. What are the Most Common Positive and Negative Words?
Using the **Bing Lexicons**, you get scores for Positive/Negative Words, these are the Top 20 most common -ve and +ve Words

```{r}
reviews_processed %>% 
  unnest_tokens(word, content) %>% 
  inner_join(get_sentiments("bing")) %>% 
  anti_join(stop_words, by="word") %>% 
  select(word, sentiment) %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup() %>% 
  group_by(sentiment)  %>% 
  top_n(20) %>% 
  ungroup() %>% 
  mutate(word = fct_reorder(word, n)) %>% 
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free") + 
  coord_flip() +
  labs(y = "Contribution to Sentiment", x="")
  
```

## 4. It is important to see which words contribute to your sentiment scores. 
What exactly contribute most the different sentiment like anger, disgust, fear etc
 
```{r fig.align='center'}
reviews_processed %>%
    unnest_tokens(word, content) %>% 
    anti_join(stop_words, by="word") %>% 
    inner_join(get_sentiments("nrc")) %>% 
    # Count by word and sentiment
    count(word, sentiment) %>% 
    filter(sentiment %in% c("anger", "disgust", "trust", "joy")) %>% 
    # Group by sentiment
    group_by(sentiment) %>%
    # Take the top 10 words for each sentiment
    top_n(10) %>%
    ungroup() %>%
    mutate(word = reorder(word, n)) %>%
    # Set up the plot with aes()
    ggplot(aes(word, n, fill=sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ sentiment, scales = "free") +
    coord_flip() +
    theme_fivethirtyeight()

```


## 5. Sentiment changes with time

```{r fig.align='center'}
sentiment_by_time <- reviews_processed %>%
    unnest_tokens(word, content) %>% 
    anti_join(stop_words, by="word") %>% 
    # Define a new column using floor_date()
    mutate(date = floor_date(at, unit = "3 months")) %>%
    # Group by date
    group_by(date) %>%
    mutate(total_words = n()) %>%
    ungroup() %>%
    # Implement sentiment analysis using the NRC lexicon
    inner_join(get_sentiments("nrc"), by="word")


sentiment_by_time %>%
    # Filter for positive and negative words
    filter(sentiment %in% c("positive", "negative", "trust", "anger")) %>%
    # Count by date, sentiment, and total_words
    count(date, sentiment, total_words) %>%
    ungroup() %>%
    mutate(percent = n / total_words) %>%
    # Set up the plot with aes()
    ggplot(aes(date, percent, color = sentiment))+
    geom_line(size = 1.5) +
    geom_smooth(method = "lm", se = FALSE, lty = 2) +
    expand_limits(y = 0) +
    theme_fivethirtyeight()
```



# PART 2

So far we’ve considered words as individual units, and considered their relationships to sentiments. However, many interesting text analyses are based on the relationships between words, e.g examining which words tend to follow others immediately


## 6. Visualizing a network of bigrams

Lets visualize all of the relationships among words simultaneously, rather than just the top few at a time.

```{r fig.align='center'}

set.seed(12345)

bigrams_ratings <- reviews_processed %>%
  unnest_tokens(bigrams, content, token = "ngrams", n = 2) %>% 
  select(bigrams, everything())
  # sample_n(10) %>% 
  # pull(bigrams)

bigrams_ratings_separated <- bigrams_ratings %>% 
  separate(bigrams, c("word1", "word2", sep = " ")) %>% 
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) %>% 
  count(word1, word2, sort = TRUE)

bigram_graph <- bigrams_ratings_separated %>% 
  filter(n > 10) %>% 
  graph_from_data_frame()


a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

### 7.1 Words preceded by Not, No, Never, Without
By performing sentiment analysis on the bigram data, we can examine how often sentiment-associated words are preceded by “not” or other negating words like "no", "Never" and "Without"


```{r fig.align='center'}
negation_words <- c("not", "no", "never", "without")
AFINN <- get_sentiments("afinn")
bigrams_ratings %>%
  separate(bigrams, into = c("word1", "word2"), sep = " ") %>% 
  filter(word1 %in% negation_words)  %>%   
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE) %>% 
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(30) %>% 
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment value * number of occurrences") +
  coord_flip() +
  labs(title = "Words Preceeded by NOT...")
  # facet_wrap(~word1, ncol = 2)
```

### 7.2 Word Cloud
Text analysis is never complete without a word cloud. `r emo::ji("smile")`

```{r}
library(wordcloud)

reviews_processed %>%
  unnest_tokens(word, content) %>% 
  anti_join(stop_words, by="word") %>% 
  count(word) %>%
  with(wordcloud(word, n, max.words = 200))

```































